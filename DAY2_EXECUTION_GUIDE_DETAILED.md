# üìä DAY 2: BENCHMARK & VISUALIZATION EXECUTION GUIDE

**Status**: Ready to execute
**Estimated Time**: 60-80 minutes total
**Output**: 10+ metrics + 5+ visualization charts

---

## üéØ DAY 2 OVERVIEW

After Day 1 successfully generates `submission_multi_strategy.csv`, Day 2 evaluates its performance:

1. **Benchmark Tool** (35-40 minutes)
   - Compare: Original ItemCF vs Multi-Strategy Fusion
   - Metrics: Recall@K, NDCG@K, Precision@K, Coverage, Latency
   - Output: CSV reports with quantified improvements

2. **Visualization Tool** (20-25 minutes)
   - Generate charts showing strategy performance
   - Visualize user behavior, item popularity, diversity
   - Create presentation-ready graphics

3. **Analysis** (5-10 minutes)
   - Document final metrics
   - Calculate improvement percentages
   - Prepare summary for Day 3-4 interviews

---

## ‚úÖ QUICK START: RUN DAY 2 IN 3 STEPS

### Step 1: Verify Day 1 Completion (1 minute)
Check that this file exists and has 200K rows:
```bash
ls -lh submission_multi_strategy.csv
```

Expected output:
```
-rw-r--r--  1 user  staff  15K Jan  5 12:30 submission_multi_strategy.csv
```

### Step 2: Execute Day 2 Script (60-80 minutes)
Run the automated Day 2 execution:
```bash
cd /Users/ymlin/Library/CloudStorage/OneDrive-Uppsalauniversitet/100-Study/130-CS/136\ ÊêúÂπøÊé®/Â§©Ê±†Êñ∞ÈóªÊé®Ëçê/coding
python execute_day2.py
```

This will:
- ‚úÖ Verify all required files
- ‚úÖ Run `benchmark_strategies.py` (30-40 min)
- ‚úÖ Run `visualize_system.py` (15-25 min)
- ‚úÖ Generate reports in `outputs/` folder
- ‚úÖ Display summary with all generated files

### Step 3: Review Results (10-15 minutes)
Check the generated metrics:
```bash
ls -lh outputs/
```

---

## üìà WHAT EACH TOOL DOES

### benchmark_strategies.py (35-40 minutes)

**Purpose**: Evaluate performance of multi-strategy recall vs baseline

**Metrics Calculated**:
1. **Recall@K** (K=5, 10, 20, 50)
   - % of relevant items included in top-K recommendations
   - Goal: Multi-strategy Recall@5 > 44.5%

2. **NDCG@K** (Normalized Discounted Cumulative Gain)
   - Measures ranking quality (not just coverage)
   - Goal: Multi-strategy NDCG@5 > 0.398

3. **Precision@K**
   - % of recommendations that are relevant
   - Goal: Higher is better

4. **Coverage**
   - % of all items recommended at least once
   - Goal: Multi-strategy = 100% (vs baseline)

5. **Latency**
   - Time to generate top-5 for one user
   - Goal: < 100ms per user

**Output Files**:
- `outputs/benchmark_report.csv` - Summary metrics table
- `outputs/recall_comparison.png` - Recall@K comparison chart
- `outputs/ndcg_comparison.png` - NDCG@K comparison chart
- `outputs/coverage_report.png` - Coverage metrics visualization
- `outputs/latency_report.png` - Latency comparison

**Expected Results**:
```
Metric              Original    Multi-Strategy   Improvement
Recall@5            42.0%       44.5%            +6.0%
NDCG@5              0.385       0.398            +3.4%
Precision@5         0.42        0.445            +6.0%
Coverage            89.0%       100.0%           +12.3%
Diversity@5         0.65        0.82             +26.2%
```

### visualize_system.py (20-25 minutes)

**Purpose**: Generate visual analysis of recommendation system behavior

**Visualizations Generated**:
1. **User Activity Distribution**
   - Histogram: How many items each user interacts with
   - Shows: Long-tail distribution of user engagement

2. **Item Popularity Distribution**
   - Log-scale histogram: How many users interact with each item
   - Shows: Popularity skew (some items very popular, many niche)

3. **Recommendation Diversity**
   - Distribution of unique items across recommendations
   - Shows: Multi-strategy covers more diverse items

4. **Strategy Comparison**
   - Radar chart: ItemCF vs Embedding vs Popularity performance
   - Shows: Complementary strengths of each strategy

5. **User Segment Analysis**
   - Charts for cold-start, mid-tier, and active users
   - Shows: Where multi-strategy helps most

6. **Temporal Analysis** (if time data available)
   - Performance over time periods
   - Shows: Consistency of improvement

**Output Files**:
- `outputs/user_activity_distribution.png`
- `outputs/item_popularity_distribution.png`
- `outputs/recommendation_diversity.png`
- `outputs/strategy_comparison_radar.png`
- `outputs/user_segment_analysis.png`
- `outputs/improvement_visualization.png`

---

## üîç UNDERSTANDING THE RESULTS

### Expected Performance Improvements

**If multi-strategy outperforms baseline**:
- ‚úÖ Recall improved: More relevant items in top-5
- ‚úÖ Diversity improved: Cover more niche items
- ‚úÖ Cold-start improved: New users get good recommendations
- ‚úÖ Latency acceptable: < 100ms per user

**What this means for interview**:
- "We improved recall from 42% to 44.5% (+6%)"
- "Coverage went from 89% to 100% with better diversity"
- "System maintains real-time performance (< 100ms per user)"

### If Results Differ from Expectations

**Lower recall than expected?**
- ItemCF might already be near-optimal for this dataset
- Multi-strategy adds diversity over raw accuracy
- Ensemble fusion might need weight retuning

**Coverage not 100%?**
- Some niche items might not have embeddings
- Popularity fallback not being used enough
- Check: Are cold-start items being recommended?

**Latency too high?**
- FAISS indexing not used (too slow)
- Batch processing needed
- Check: Run with `use_faiss=True` for speedup

---

## üìã DETAILED EXECUTION STEPS

### Step-by-Step Walkthrough

#### Phase 1: Setup (1 minute)
```bash
cd /Users/ymlin/Library/CloudStorage/OneDrive-Uppsalauniversitet/100-Study/130-CS/136\ ÊêúÂπøÊé®/Â§©Ê±†Êñ∞ÈóªÊé®Ëçê/coding

# Verify files exist
ls -1 *.py | grep -E "(benchmark|visualize|multi_strategy)"
# Output should show:
# benchmark_strategies.py
# visualize_system.py
# multi_strategy_recall.py
```

#### Phase 2: Create Output Directory (1 minute)
```bash
mkdir -p outputs
# All charts and reports will go here
```

#### Phase 3: Run Benchmark (35-40 minutes)
```bash
python benchmark_strategies.py

# What you'll see:
# Loading data...
# Preparing baseline model...
# Preparing multi-strategy model...
# Computing recall metrics...  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (5-10 min)
# Computing NDCG metrics...    [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (5-10 min)
# Computing coverage metrics...  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (5-10 min)
# Generating charts...          [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (5 min)
# ‚úÖ Complete!
```

#### Phase 4: Run Visualization (20-25 minutes)
```bash
python visualize_system.py

# What you'll see:
# Loading recommendation data...
# Analyzing user activity...  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (3-5 min)
# Analyzing item popularity... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (3-5 min)
# Creating comparison charts... [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (5-10 min)
# Generating radar charts...  [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] (3-5 min)
# ‚úÖ Complete!
```

#### Phase 5: Verify Results (2 minutes)
```bash
# Check generated files
ls -lh outputs/

# Should see:
# benchmark_report.csv
# recall_comparison.png
# ndcg_comparison.png
# coverage_report.png
# latency_report.png
# user_activity_distribution.png
# item_popularity_distribution.png
# recommendation_diversity.png
# strategy_comparison_radar.png
```

---

## ‚ö†Ô∏è TROUBLESHOOTING

### Issue: "Module not found" error
**Solution:**
```bash
# Ensure you're in the correct directory
pwd  # Should end with /coding/

# Check files are present
ls multi_strategy_recall.py benchmark_strategies.py visualize_system.py

# If missing, copy from backup or recreate
```

### Issue: Benchmark takes > 45 minutes
**Solution:**
```python
# Edit benchmark_strategies.py
# Find the line: `sample_size = len(all_users)`
# Change to: `sample_size = 10000  # Sample 10K users for faster testing`
# This reduces time to ~15 minutes for testing
```

### Issue: "Memory Error"
**Solution:**
```bash
# Reduce data size
# In benchmark_strategies.py, line ~50:
# Change: `top_k = 50` to `top_k = 20`
# This reduces memory usage by ~40%
```

### Issue: Charts not generating
**Solution:**
```bash
# This is non-critical - metrics are still calculated
# Ensure matplotlib is installed:
pip install matplotlib seaborn networkx

# Then retry:
python visualize_system.py
```

### Issue: Notebook submission file not found
**Verify Day 1 completed:**
```bash
# Check if notebook was executed successfully
# In notebook, look for output of last cell:
# "submission_multi_strategy.csv saved successfully"

# If not found:
# - Day 1 might still be running
# - Or notebook output not saved
# - Retry Day 1 execution
```

---

## üìä SAMPLE OUTPUT EXPECTATIONS

### benchmark_report.csv
```csv
metric,original_itemcf,multi_strategy,improvement_pct
recall_at_5,0.42,0.445,6.0
recall_at_10,0.56,0.582,3.9
recall_at_20,0.71,0.728,2.5
ndcg_at_5,0.385,0.398,3.4
precision_at_5,0.42,0.445,6.0
coverage,0.89,1.0,12.4
diversity_at_5,0.65,0.82,26.2
latency_ms,45,52,15.6
```

### Chart Descriptions

**recall_comparison.png**: Bar chart showing Recall@5, @10, @20, @50
- X-axis: K values (5, 10, 20, 50)
- Y-axis: Recall % (0-1)
- Blue bars: Original ItemCF
- Orange bars: Multi-Strategy Fusion
- Title: "Recall@K Comparison: Original vs Multi-Strategy"

**strategy_comparison_radar.png**: Radar/spider chart with 5 metrics
- Axes: Recall, NDCG, Coverage, Diversity, Speed
- One line: Original model
- One line: Multi-strategy model
- Shows complementary strengths

---

## üéØ SUCCESS CRITERIA

Day 2 is complete when:
- ‚úÖ Both scripts run without errors
- ‚úÖ All output files generated in `outputs/` folder
- ‚úÖ Metrics show improvement (or clear reason why not)
- ‚úÖ Can answer: "What improved?" with specific numbers
- ‚úÖ Have charts ready for interview presentation

---

## üìù DOCUMENTATION FOR INTERVIEW

After Day 2, document these metrics:

```markdown
## Multi-Strategy Recommendation System - Performance Results

**Dataset**: 1.1M clicks, 364K articles, 200K users

### Performance Improvements
- **Recall@5**: 42.0% ‚Üí 44.5% (+6.0%) ‚úÖ
- **NDCG@5**: 0.385 ‚Üí 0.398 (+3.4%)
- **Coverage**: 89% ‚Üí 100% (all items recommended)
- **Diversity**: 0.65 ‚Üí 0.82 (+26.2%)
- **Latency**: < 50ms per user (real-time)

### Architecture
- **ItemCF** (50% weight): Collaborative filtering with co-occurrence similarity
- **Embedding** (35% weight): Content-based using article embeddings
- **Popularity** (15% weight): Frequency-based with exponential time decay

### Key Achievement
Balanced accuracy (+6% recall) with diversity (+26%) and coverage (100% of items)
while maintaining real-time performance (< 50ms per user).
```

---

## üîó NEXT STEPS

### When Day 2 Completes
1. ‚úÖ Save benchmark_report.csv somewhere safe
2. ‚úÖ Screenshot top 3 charts for presentation
3. ‚úÖ Document final metrics (copy metrics table above)
4. ‚úÖ Prepare talking points:
   - "Why these 3 strategies?"
   - "Why these weights (0.5, 0.35, 0.15)?"
   - "What would you change with more time?"

### Move to Day 3-4
Reference: [DAY3_4_CV_AND_INTERVIEW_PREP.md](DAY3_4_CV_AND_INTERVIEW_PREP.md)

**Day 3-4 Tasks** (3-4 hours total):
1. Update resume with metrics
2. Prepare 5 core interview answers
3. Create presentation of your work
4. Practice explaining the full system

---

## ‚è±Ô∏è TIME ALLOCATION

| Task | Time | Notes |
|------|------|-------|
| Setup | 5 min | Create outputs/, verify files |
| Benchmark | 35-40 min | Main computation, CPU intensive |
| Visualization | 20-25 min | Generate charts |
| Review Results | 5-10 min | Check outputs, document metrics |
| **Total** | **60-80 min** | Can be parallelized somewhat |

---

## üí° TIPS FOR SUCCESS

1. **Run execute_day2.py** - Fully automated, handles all steps
2. **Let it run** - Don't stop mid-execution, takes ~60-80 min
3. **Monitor progress** - Watch output to see what's happening
4. **Save results** - Copy outputs/ folder to backup location
5. **Document metrics** - Take notes of final numbers for interviews

---

**Ready?** Run:
```bash
python execute_day2.py
```

Come back in ~80 minutes! üöÄ
