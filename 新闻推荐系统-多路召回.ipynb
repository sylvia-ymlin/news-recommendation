{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GokwvhDvFXTO"
   },
   "source": [
    "多路召回：采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用\n",
    "\n",
    "多路召回是计算速度和召回率之间的平衡：各种简单的召回策略保证候选集的快速召回；从不同角度设计召回策略，保证召回率接近理想状态。在多路召回中，每个策略之间毫不相干，所以可以写并发多线程同时进行，提高效率。\n",
    "\n",
    "具体使用哪些召回策略**与业务强相关**，针对新闻推荐：\n",
    "- 热门新闻\n",
    "- 作者召回\n",
    "- 关键词召回\n",
    "- 主题召回\n",
    "- 协同过滤召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1035,
     "status": "ok",
     "timestamp": 1759344875660,
     "user": {
      "displayName": "Yangmei Lin",
      "userId": "03958463100219057182"
     },
     "user_tz": -120
    },
    "id": "XOJ5DKJkK8yt",
    "outputId": "a39529d6-f120-469a-bb0b-8553db251626"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = Path.cwd()\n",
    "\n",
    "data_path = str(base_path / 'data') + '/'\n",
    "\n",
    "save_path = str(base_path / 'temp_results') + '/'\n",
    "\n",
    "Path(data_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDjCxnOEX6I9"
   },
   "source": [
    "# 导包\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11628,
     "status": "ok",
     "timestamp": 1759344887301,
     "user": {
      "displayName": "Yangmei Lin",
      "userId": "03958463100219057182"
     },
     "user_tz": -120
    },
    "id": "zkW1blrjZamP",
    "outputId": "b42b1c3a-9ce0-4222-a15f-dacf0c13e22b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow deepctr deepmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9106,
     "status": "ok",
     "timestamp": 1759344896408,
     "user": {
      "displayName": "Yangmei Lin",
      "userId": "03958463100219057182"
     },
     "user_tz": -120
    },
    "id": "0OvmUlfOZ9s4",
    "outputId": "8e76420e-e0f6-4eeb-b205-c77fef469173"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R6biyyGGW9Vi"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     27\u001b[39m logger.setLevel(logging.INFO)\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfaiss\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "\n",
    "subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'faiss-cpu', 'tqdm'])\n",
    "\n",
    "\n",
    "\n",
    "import os, math, warnings, math, pickle, random\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import logging\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "import faiss\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5mmSXb_Kdw7W"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_path = Path.cwd()\n",
    "\n",
    "data_path = str(base_path / 'data') + '/'\n",
    "\n",
    "save_path = str(base_path / 'temp_results') + '/'\n",
    "\n",
    "Path(data_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Path(save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 做召回评估的一个标志, 如果不进行评估就是直接使用全量数据进行召回\n",
    "\n",
    "metric_recall = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTsLdpLxd9mT"
   },
   "source": [
    "# 读取数据\n",
    "一般推荐系统比赛中，会有三种方式读取数据\n",
    "1. Debug 模式：使用少量数据做调试。先搭建一个简易的 baseline 并跑通，保证 baseline 代码没有问题。\n",
    "2. 线下验证：需要留出一部分数据，测试模型的效果，选择合适的超参数。我们只加载 train_click_log，然后将这个数据集划分为训练集和验证集。\n",
    "3. 线上模式：用整个（train_click_log+test_click_log）训练模型，预测结果提交到线上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMyNVvMhd57Y"
   },
   "outputs": [],
   "source": [
    "# debug mode\n",
    "def get_all_click_sample(data_path, sample_nums=10000):\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    all_user_ids = all_click.user_id.unique() # 获得去重的 user_id\n",
    "\n",
    "    # randomly choose, don't put back\n",
    "    random.seed(42)\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_nums, replace=False)\n",
    "    all_click = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "\n",
    "    # drop repeated records\n",
    "    all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "\n",
    "    return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiQZYEUyigvw"
   },
   "outputs": [],
   "source": [
    "# online and off-line\n",
    "def get_all_click_df(data_path, offline=True):\n",
    "  if offline:\n",
    "    all_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "  else:\n",
    "    trn_click = pd.read_csv(data_path + 'train_click_log.csv')\n",
    "    tst_click = pd.read_csv(data_path + 'testA_click_log.csv')\n",
    "\n",
    "    all_click = pd.concat([trn_click, tst_click])\n",
    "\n",
    "  # 去重\n",
    "  all_click = all_click.drop_duplicates((['user_id', 'click_article_id', 'click_timestamp']))\n",
    "\n",
    "  return all_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6E7X3-ni5Qr"
   },
   "outputs": [],
   "source": [
    "# load from article.csv\n",
    "def get_item_info_df(data_path):\n",
    "  item_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "\n",
    "  # 为了方便与训练集中的 click_article_id 对应，需要把 article_id 修改\n",
    "  item_info_df = item_info_df.rename(columns={'article_id': 'click_article_id'})\n",
    "\n",
    "  return item_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rv5MGcOjuYw"
   },
   "outputs": [],
   "source": [
    "# load embedding\n",
    "def get_item_emb_dict(data_path, save_path):\n",
    "  item_emb_df = pd.read_csv(data_path + 'articles_emb.csv')\n",
    "\n",
    "  # 通过列名里包含 'emb' 来定位向量的每一维（如 emb_0, emb_1, ...）\n",
    "  item_emb_cols = [x for x in item_emb_df.columns if 'emb' in x]\n",
    "  # 转成连续内存的 NumPy 矩阵\n",
    "  item_emb_np = np.ascontiguousarray(item_emb_df[item_emb_cols])\n",
    "\n",
    "  # L2 归一化\n",
    "  # 用每行的范数把该行除一下，使每个向量长度为 1，后续 内积 就是余弦相似度\n",
    "  # 若用 FAISS 的 IndexFlatIP (内积作相似度)，必须先做这一步\n",
    "  item_emb_np = item_emb_np / np.linalg.norm(item_emb_np, axis=1, keepdims=True)\n",
    "  # 组装成字典：生成 {article_id → 向量} 的查找表\n",
    "  item_emb_dict = dict(zip(item_emb_df['article_id'], item_emb_np))\n",
    "  # 持久化到本地\n",
    "  pickle.dump(item_emb_dict, open(save_path + 'item_content_emb.pkl', 'wb'))\n",
    "\n",
    "  return item_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a1S99ztXmzfu"
   },
   "outputs": [],
   "source": [
    "# 最小-最大归一化\n",
    "max_min_scaler = lambda x: (x - np.min(x))/(np.max(x) - np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZtYOmQ3m8eN"
   },
   "outputs": [],
   "source": [
    "all_click_df = get_all_click_df(data_path, offline=False)\n",
    "# 对时间戳归一化\n",
    "all_click_df['click_timestamp'] = all_click_df[['click_timestamp']].apply(max_min_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChwFkR_dnPXc"
   },
   "outputs": [],
   "source": [
    "item_info_df = get_item_info_df(data_path)\n",
    "# 对文章创建时间归一化\n",
    "item_info_df['created_at_ts'] = item_info_df[['created_at_ts']].apply(max_min_scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hsiuoaqtnQGR"
   },
   "outputs": [],
   "source": [
    "item_emb_dict = get_item_emb_dict(data_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy7-bdP-nU77"
   },
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JkMMnw4JnW6P"
   },
   "outputs": [],
   "source": [
    "# 获取 用户-文章-时间\n",
    "#  {user1: [(item1, time1), (item2, time2)..]...}\n",
    "# 用于基于关联规则的 用户协同过滤\n",
    "def get_user_item_time(click_df):\n",
    "  click_df = click_df.sort_values('click_timestamp')\n",
    "\n",
    "  def make_item_time_pair(df):\n",
    "    return list(zip(df['click_article_id'], df['click_timestamp']))\n",
    "\n",
    "  # group by user_id, takeout 'click_article_id', 'click_timestamp'\n",
    "  user_item_time_df = click_df.groupby('user_id')[['click_article_id', 'click_timestamp']] \\\n",
    "      .apply(lambda x: make_item_time_pair(x)).reset_index().rename(columns={0: 'item_time_list'})\n",
    "\n",
    "  # 转换为字典\n",
    "  user_item_time_dict = dict(zip(user_item_time_df['user_id'], user_item_time_df['item_time_list']))\n",
    "\n",
    "  return user_item_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEXr0V21oYpq"
   },
   "outputs": [],
   "source": [
    "# 获取 文章-用户-时间\n",
    "# {item1: [(user1, time1), (user2, time2)...]...}\n",
    "# 用于基于关联规则的 物品协同过滤\n",
    "def get_item_user_time(click_df):\n",
    "  click_df = click_df.sort_values('click_timestamp')\n",
    "\n",
    "  def make_user_time_pair(df):\n",
    "    return list(zip(df['user_id'], df['click_timestamp']))\n",
    "\n",
    "  item_user_time_df = click_df.groupby('click_article_id')[['user_id', 'click_timestamp']] \\\n",
    "    .apply(lambda x: make_user_time_pair(x)).reset_index().rename(columns={0: 'user_time_list'})\n",
    "\n",
    "  item_user_time_dict = dict(zip(item_user_time_df['click_article_id'], item_user_time_df['user_time_list']))\n",
    "\n",
    "  return item_user_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fhl5pPUDn-Eu"
   },
   "outputs": [],
   "source": [
    "# 获取当前数据的历史点击和最后一次点击\n",
    "# 用户评估召回结果\n",
    "# 用于特征工程\n",
    "# 用于制作标签，转成监督学习测试集\n",
    "def get_hist_and_last_click(click_df):\n",
    "  # 按用户排序，同一个用户按照时间升序\n",
    "  all_click = click_df.sort_values(by=['user_id', 'click_timestamp'])\n",
    "  # 获取每个用户的最后一次点击\n",
    "  click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "  # 用户最后一次点击要从日志中剔除，但是如果用户本身只有一次点击，为了避免数据缺失，则不剔除\n",
    "  def hist_func(user_df):\n",
    "    if len(user_df) == 1:\n",
    "      return user_df\n",
    "    else:\n",
    "      return user_df[:-1]\n",
    "\n",
    "  click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "  return click_hist_df, click_last_df\n",
    "\n",
    "  # 获取每个用户的历史点击\n",
    "  # drop=True: 把当前索引丢弃，重新生成从 0 开始的连续行号索引\n",
    "  click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "  return click_hist_df, click_last_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3QrOoT_qKHn"
   },
   "outputs": [],
   "source": [
    "# 获取文章属性\n",
    "def get_item_info_dict(item_info_df):\n",
    "  # 将文章和文章各个属性都做一个字典\n",
    "  item_type_dict = dict(zip(item_info_df['click_article_id'], item_info_df['category_id']))\n",
    "  item_words_dict = dict(zip(item_info_df['click_article_id'], item_info_df['words_count']))\n",
    "  item_created_time_dict = dict(zip(item_info_df['click_article_id'], item_info_df['created_at_ts']))\n",
    "\n",
    "  return item_type_dict, item_words_dict, item_created_time_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTaNGycmq9xu"
   },
   "outputs": [],
   "source": [
    "# 获取用户历史点击文章的信息\n",
    "def get_user_hist_item_info_dict(all_click):\n",
    "  # 用户历史点击的文章的类型\n",
    "  user_hist_item_typs = all_click.groupby('user_id')['category_id'].agg(set).reset_index()\n",
    "  # 转为字典\n",
    "  user_hist_item_typs_dict = dict(zip(user_hist_item_typs['user_id'], user_hist_item_typs['category_id']))\n",
    "\n",
    "  # 文章的集合\n",
    "  user_hist_item_ids_dict = all_click.groupby('user_id')['click_article_id'].agg(set).reset_index()\n",
    "  user_hist_item_ids_dict = dict(zip(user_hist_item_ids_dict['user_id'], user_hist_item_ids_dict['click_article_id']))\n",
    "\n",
    "  # 文章平均字数\n",
    "  user_hist_item_words = all_click.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "  user_hist_item_words_dict = dict(zip(user_hist_item_words['user_id'], user_hist_item_words['words_count']))\n",
    "\n",
    "  # 最后一次点击的文章的创建时间\n",
    "  all_click_ = all_click.sort_values('click_timestamp')\n",
    "  user_last_item_created_time = all_click_.groupby('user_id')['created_at_ts'].apply(lambda x: x.iloc[-1]).reset_index()\n",
    "  user_last_item_created_time_dict = dict(zip(user_last_item_created_time['user_id'], \\\n",
    "                                                user_last_item_created_time['created_at_ts']))\n",
    "\n",
    "  return user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hwcTbOIsHO6"
   },
   "outputs": [],
   "source": [
    "# 点击次数最多的topk个文章\n",
    "def get_item_topk_click(click_df, k):\n",
    "  topk_click = click_df['click_article_id'].value_counts().index[:k]\n",
    "  return topk_click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wimufvQEsP_U"
   },
   "source": [
    "# 定义多路召回字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXbxQxuAsamy"
   },
   "outputs": [],
   "source": [
    "# 这里实现了五个召回通道\n",
    "user_multi_recall_dict =  {'itemcf_sim_itemcf_recall': {},\n",
    "                           'embedding_sim_item_recall': {},\n",
    "                           'youtubednn_recall': {},\n",
    "                           'youtubednn_usercf_recall': {},\n",
    "                           'cold_start_recall': {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mC-yqILIsTiU"
   },
   "outputs": [],
   "source": [
    "item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ZjPmwhEslQf"
   },
   "outputs": [],
   "source": [
    "# 提取数据，offline, 最后一次点击用作召回评估\n",
    "trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj-6tP3ZtAq0"
   },
   "source": [
    "# 召回效果评估函数\n",
    "召回的结果决定了最终排序的上限。根据评估结果，对召回方法或者参数进行调整，以获得更好的召回效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSs6pKjZtRc1"
   },
   "outputs": [],
   "source": [
    "# 依次评估召回中前 10， 20， 30， 40， 50 个文章中的击中率\n",
    "# 越靠前，说明召回的效果越好，不是一定要控制召回的数量，但是如果多篇召回都没有命中，召回算法是有问题的\n",
    "def metrics_recall(user_recall_items_dict, trn_last_click_df, topk=5):\n",
    "  # 需要 (user_id, click_article_id)\n",
    "  last_click_item_dict = dict(zip(trn_last_click_df['user_id'], trn_last_click_df['click_article_id']))\n",
    "\n",
    "  user_num = len(user_recall_items_dict)\n",
    "\n",
    "  for k in range(10, topk+1, 10):\n",
    "    hit_num = 0\n",
    "    for user, item_list in user_recall_items_dict.items():\n",
    "      # 获取前 k 召回结果\n",
    "      tmp_recall_items = [x[0] for x in user_recall_items_dict[user][:k]]\n",
    "      if last_click_item_dict[user] in set(tmp_recall_items):\n",
    "        hit_num += 1\n",
    "\n",
    "  # 有多少用户命中\n",
    "  hit_rate = round(hit_num * 100.0 / user_num, 2)\n",
    "  logger.info('topk: {}, hit_num: {}, hit_rate: {}'.format(k, hit_num, hit_rate))\n",
    "  return hit_rate # 最后返回的是 传入参数 topk 的命中率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4cT7Dlw5vMk"
   },
   "source": [
    "# 计算相似矩阵\n",
    "通过**协同过滤**以及**向量检索**得到相似性矩阵。相似矩阵主要分为 user2user 和 item2item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pL5f5Sqn7lpn"
   },
   "source": [
    "## ItemCF: i2i_sim\n",
    "借鉴KDD2020的去偏商品推荐，在计算item2item相似性矩阵时，使用关联规则。\n",
    "- 用户点击的时间权重：距离“现在”越近、或两次点击时间间隔越小的共现对，权重越大\n",
    "- 用户点击的顺序权重：相邻或近邻的物品对更相关\n",
    "- 文章创建的时间权重：越新（或按策略）→ 关系更强，缓解“老热”偏置\n",
    "\n",
    "从用户点击序列里，统计物品对 (i, j) 的共现强度，叠加顺序权重、点击时间权重、文章创建时间权重，再做一次余弦式归一化，得到 $S_{ij}$ 作为 item2item 相似度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 388772,
     "status": "ok",
     "timestamp": 1759345410847,
     "user": {
      "displayName": "Yangmei Lin",
      "userId": "03958463100219057182"
     },
     "user_tz": -120
    },
    "id": "zfNZZvbf559w",
    "outputId": "92a66acf-d45d-43b1-bd5a-444b5e431093"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [05:25<00:00, 768.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def itemcf_sim(df, item_created_time_dict):\n",
    "  # item_created_time_dict: 其实就是索引\n",
    "  user_item_time_dict = get_user_item_time(df)\n",
    "\n",
    "  # 存放物品相似度\n",
    "  i2i_sim = {}\n",
    "  # 统计物品被点击次数\n",
    "  item_cnt = defaultdict(int)\n",
    "  for user, item_time_list in tqdm(user_item_time_dict.items()):\n",
    "    # 考虑时间因素 [loc1 其实是 index]\n",
    "    for loc1, (item1, time1) in enumerate(item_time_list):\n",
    "      item_cnt[item1] += 1\n",
    "      i2i_sim.setdefault(item1, {})\n",
    "      for loc2, (item2, time2) in enumerate(item_time_list):\n",
    "        if item1 == item2:\n",
    "          continue\n",
    "        # 文章点击顺序: 顺序偏好，sim(i, j) != sim(j, i)\n",
    "        loc_alpha = 1.0 if loc1 > loc2 else 0.7\n",
    "        # 位置权重: 相邻（间距=1）权重最大 1，间距越远越小\n",
    "        loc_weight = loc_alpha * (0.9 ** (np.abs(loc1 - loc2) - 1))\n",
    "        # 时间权重\n",
    "        click_time_weight = np.exp(0.7 ** np.abs(time1 - time2))\n",
    "        # 文章创建时间权重\n",
    "        created_time_weight = np.exp(0.7 ** np.abs(item_created_time_dict[item1] - item_created_time_dict[item2]))\n",
    "\n",
    "        # item1 -> item2\n",
    "        i2i_sim[item1].setdefault(item2, 0)\n",
    "        # 融合，然后做归一化\n",
    "        i2i_sim[item1][item2] += loc_weight * click_time_weight * created_time_weight / math.log(len(item_time_list) + 1)\n",
    "\n",
    "  i2i_sim_ = i2i_sim.copy()\n",
    "  for i, related_items in i2i_sim.items():\n",
    "      for j, wij in related_items.items():\n",
    "          i2i_sim_[i][j] = wij / math.sqrt(item_cnt[i] * item_cnt[j])\n",
    "\n",
    "  # 将得到的相似性矩阵保存到本地\n",
    "  pickle.dump(i2i_sim_, open(save_path + 'itemcf_i2i_sim.pkl', 'wb'))\n",
    "\n",
    "  return i2i_sim_\n",
    "\n",
    "i2i_sim = itemcf_sim(all_click_df, item_created_time_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbspEN0z_m1-"
   },
   "source": [
    "## UserCF u2u_sim\n",
    "计算用户相似度的时候，可以考虑用户活跃度权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o3LiVfO9__sB"
   },
   "outputs": [],
   "source": [
    "def get_user_activate_degree_dict(all_click_df):\n",
    "  cnt = all_click_df.groupby('user_id')['click_article_id'].count().reset_index(name='click_cnt')\n",
    "  mm = MinMaxScaler()\n",
    "  cnt['activity_score'] = mm.fit_transform(cnt[['click_cnt']])\n",
    "  user_activate_degree_dict = dict(zip(cnt['user_id'], cnt['activity_score']))\n",
    "\n",
    "  return user_activate_degree_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9465,
     "status": "ok",
     "timestamp": 1759346024148,
     "user": {
      "displayName": "Yangmei Lin",
      "userId": "03958463100219057182"
     },
     "user_tz": -120
    },
    "id": "YRPrQa2gBftq",
    "outputId": "54aac967-0e1a-4e1e-c772-f6a81e9c4d4e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6411/6411 [00:04<00:00, 1323.30it/s]\n"
     ]
    }
   ],
   "source": [
    "def usercf_sim(all_click_df, user_activate_degree_dict):\n",
    "  item_user_time_dict = get_item_user_time(all_click_df)\n",
    "\n",
    "  u2u_sim = {}\n",
    "  user_cnt = defaultdict(int)\n",
    "  for item, user_time_list in tqdm(item_user_time_dict.items()):\n",
    "    for u, click_time in user_time_list:\n",
    "      user_cnt[u] += 1\n",
    "      u2u_sim.setdefault(u, {})\n",
    "\n",
    "      for v, click_time in user_time_list:\n",
    "        u2u_sim[u].setdefault(v, 0)\n",
    "        if u == v:\n",
    "          continue\n",
    "\n",
    "        # 用户活跃度差值有关\n",
    "        activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])\n",
    "        u2u_sim[u][v] += activate_weight / math.log(len(user_time_list) + 1)\n",
    "\n",
    "  u2u_sim_ = u2u_sim.copy()\n",
    "  for u, related_users in u2u_sim.items():\n",
    "    for v, wij in related_users.items():\n",
    "      u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])\n",
    "\n",
    "  # save\n",
    "  pickle.dump(u2u_sim_, open(save_path + 'usercf_u2u_sim.pkl', 'wb'))\n",
    "\n",
    "  return u2u_sim_\n",
    "\n",
    "sample = get_all_click_sample(data_path)\n",
    "user_activate_degree_dict = get_user_activate_degree_dict(sample)\n",
    "u2u_sim = usercf_sim(sample, user_activate_degree_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-otKwk-cCvlw"
   },
   "source": [
    "## item embedding sim\n",
    "使用 embedding 计算 item 之间的相似度，主要是为了做冷启动。\n",
    "\n",
    "Faiss 是 Facebook 的 AI 团队开源的一套用于做聚类或者相似性搜索的软件库。Faiss 工具包一般用于推荐系统中向量召回部分，加速计算某个查询向量最相似的topk个索引向量 (ANN，近似临查找）\n",
    "\n",
    "faiss查询的原理：\n",
    "- PCA降维算法 https://www.cnblogs.com/pinard/p/6239403.html\n",
    "- PQ编码 http://www.fabwrite.com/productquantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZyXvj-vRF57w"
   },
   "outputs": [],
   "source": [
    "def embedding_sim(click_df, item_emb_dict, save_path, topk):\n",
    "  # click_df: 计算哪些物品的相似度\n",
    "  # item_emb_df：物品的 embedding\n",
    "  # topk: 每个物品计算 topk 相似\n",
    "\n",
    "  # 建立文章索引和文章id的字典映射\n",
    "  ids = np.array(list(item_emb_dict.keys()))\n",
    "  item_emb_dict = np.ascontiguousarray(np.stack([item_emb_dict[i] for i in ids]).astype(np.float32))\n",
    "  item_idx_2_rawid_dict = dict(enumerate(ids))\n",
    "\n",
    "  # 建立faiss索引\n",
    "  item_index = faiss.IndexFlatIP(item_emb_dict.shape[1])\n",
    "  item_index.add(item_emb_dict)\n",
    "\n",
    "  # 相似度查询，给每个索引位置上的向量返回topk个item以及相似度\n",
    "  sim, idx = item_index.search(item_emb_dict, topk) # 返回的是列表\n",
    "\n",
    "  # 将向量检索的结果，保存为原始 id 的对应关系\n",
    "  item_sim_dict = collections.defaultdict(dict)\n",
    "  for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(item_emb_dict)), sim, idx)):\n",
    "    target_raw_id = item_idx_2_rawid_dict[target_idx]\n",
    "    # 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1\n",
    "    for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "        rele_raw_id = item_idx_2_rawid_dict[rele_idx]\n",
    "        item_sim_dict[target_raw_id][rele_raw_id] = item_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "\n",
    "  pickle.dump(item_sim_dict, open(save_path + 'emb_i2i_sim.pkl', 'wb'))\n",
    "\n",
    "  return item_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 626416,
     "status": "error",
     "timestamp": 1759348350244,
     "user": {
      "displayName": "Yangmei Lin",
      "userId": "03958463100219057182"
     },
     "user_tz": -120
    },
    "id": "Gh9MJu2GH1p3",
    "outputId": "d6a794d6-f7a7-4dee-b076-863d719ab53f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1989698759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb_i2i_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_sim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_click_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem_emb_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# topk可以自行设置\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipython-input-2023451129.py\u001b[0m in \u001b[0;36membedding_sim\u001b[0;34m(click_df, item_emb_dict, save_path, topk)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m   \u001b[0;31m# 将向量检索的结果，保存为原始 id 的对应关系\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mitem_sim_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtarget_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_value_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrele_idx_list\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem_emb_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtarget_raw_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_idx_2_rawid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "emb_i2i_sim = embedding_sim(all_click_df, item_emb_dict, save_path, topk=10) # topk可以自行设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFqkav4QIIM5"
   },
   "source": [
    "# 召回\n",
    "- Youtube DNN 召回：类似矩阵分解的思路，先计算出用户和文章的embedding之后，就可以直接算用户和文章的相似度， 根据这个相似度进行推荐\n",
    "\n",
    "- 基于文章的召回\n",
    "  - 文章的协同过滤\n",
    "  - 基于文章embedding的召回\n",
    "  \n",
    "- 基于用户的召回\n",
    "  - 用户的协同过滤\n",
    "  - 用户embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgP1Ql3qgTlt"
   },
   "source": [
    "## Youtube DNN 召回\n",
    "### 数据准备\n",
    "根据用户的点击序列，用滑窗方式构造训练集和测试集，同时加上负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ruQbTT8IHqL"
   },
   "outputs": [],
   "source": [
    "# 获取双塔召回时的训练验证数据\n",
    "# negsample指的是通过滑窗构建样本的时候，负样本的数量\n",
    "def gen_data_set(data, negsample=0):\n",
    "    # 按时间/点击顺序排序\n",
    "    data.sort_values(\"click_timestamp\", inplace=True)\n",
    "    # 获取所有文章 id\n",
    "    item_ids = data['click_article_id'].unique()\n",
    "\n",
    "    # 存放训练集\n",
    "    train_set = []\n",
    "    # 存放测试集\n",
    "    test_set = []\n",
    "    # 按用户分组，逐个用户处理\n",
    "    for reviewerID, hist in tqdm(data.groupby('user_id'), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "        # 正样本：用户点击过的文章\n",
    "        pos_list = hist['click_article_id'].tolist()\n",
    "\n",
    "        # 需要做负采样\n",
    "        if negsample > 0:\n",
    "            # 用户没有点击过的文章列表\n",
    "            candidate_set = list(set(item_ids) - set(pos_list))\n",
    "            # 从用户没有点击过的文章里抽选负样本，根据正样本的数量，抽取标准是每个正样本对应 negsample 个负样本\n",
    "            neg_list = np.random.choice(candidate_set,size=len(pos_list)*negsample,replace=True)  # 对于每个正样本，选择n个负样本\n",
    "\n",
    "        # 如果用户只点击过一篇文章，则该文章需要也放入训练集，否则会有信息缺失\n",
    "        if len(pos_list) == 1:\n",
    "            train_set.append((reviewerID, [pos_list[0]], pos_list[0],1,len(pos_list)))\n",
    "            test_set.append((reviewerID, [pos_list[0]], pos_list[0],1,len(pos_list)))\n",
    "\n",
    "        # 滑窗构造正负样本，点击次数大于 1；因为用户点击是一个时间序列，从用户点击的序列中，构建多个正样本\n",
    "        '''\n",
    "          用户点击 [A, B, C, D]\n",
    "          不滑窗: 历史=[A,B,C] → 预测 D\n",
    "          滑窗：\n",
    "            历史=[A]   → 预测 B\n",
    "            历史=[A,B] → 预测 C\n",
    "            历史=[A,B,C] → 预测 D\n",
    "        '''\n",
    "        for i in range(1, len(pos_list)):\n",
    "            # hist: 历史，前 i 篇\n",
    "            # pos_item：预测目标，第 i 篇\n",
    "\n",
    "            hist = pos_list[:i]\n",
    "\n",
    "            # 不是最后一次点击\n",
    "            if i != len(pos_list) - 1:\n",
    "                # 添加正样本\n",
    "                train_set.append((reviewerID, hist[::-1], pos_list[i], 1, len(hist[::-1])))  # 正样本 [user_id, his_item, pos_item, label, len(his_item)]\n",
    "                # 添加 n 个负样本\n",
    "                for negi in range(negsample):\n",
    "                    train_set.append((reviewerID, hist[::-1], neg_list[i*negsample+negi], 0,len(hist[::-1]))) # 负样本 [user_id, his_item, neg_item, label, len(his_item)]\n",
    "            # 最后一次点击，测试集\n",
    "            else:\n",
    "                test_set.append((reviewerID, hist[::-1], pos_list[i],1,len(hist[::-1])))\n",
    "\n",
    "    # 打乱样本顺序\n",
    "    random.shuffle(train_set)\n",
    "    random.shuffle(test_set)\n",
    "\n",
    "    return train_set, test_set\n",
    "\n",
    "# 将输入的数据进行padding，使得序列特征的长度都一致\n",
    "def gen_model_input(train_set,user_profile,seq_max_len):\n",
    "\n",
    "    train_uid = np.array([line[0] for line in train_set])\n",
    "    train_seq = [line[1] for line in train_set]\n",
    "    train_iid = np.array([line[2] for line in train_set])\n",
    "    train_label = np.array([line[3] for line in train_set])\n",
    "    train_hist_len = np.array([line[4] for line in train_set])\n",
    "\n",
    "    # 因为每个用户历史点击数不一样，需要统一长度，短的补0，长的截断。\n",
    "    train_seq_pad = pad_sequences(train_seq, maxlen=seq_max_len, padding='post', truncating='post', value=0)\n",
    "    # 组装为字典\n",
    "    train_model_input = {\"user_id\": train_uid, \"click_article_id\": train_iid, \"hist_article_id\": train_seq_pad,\n",
    "                         \"hist_len\": train_hist_len}\n",
    "    # 返回输入特征，对应标签\n",
    "    return train_model_input, train_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ns8YjaATkUbw"
   },
   "source": [
    "\n",
    "1.   训练 YouTubeDNN 模型\n",
    "2.   提取用户和物品 embedding\n",
    "3.   用 FAISS 做最近邻搜索，实现用户 → 物品召回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDDMjh4Oj6ho"
   },
   "outputs": [],
   "source": [
    "# funrec youtubeDNN召回\n",
    "def youtubednn_u2i_dict(data, topk=20):\n",
    "    \"\"\"\n",
    "    使用 FunRec 的 YouTubeDNN 两塔模型进行召回，保持与当前逻辑一致的预处理：\n",
    "    - 标签/目标为正样本采样（sampled softmax 内部使用 item_id 作为 label）\n",
    "    - 通过滑窗构造训练/测试样本，使用最近序列作为测试\n",
    "    - 历史序列长度固定为 SEQ_LEN，并做 post-padding\n",
    "    - 训练完成后提取 user/item embedding，使用 FAISS 基于内积做 TopK 近邻召回\n",
    "    - 返回 {user_raw_id: [(item_raw_id, score), ...]} 的召回结果字典\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    from funrec.features.feature_column import FeatureColumn\n",
    "    from funrec.training.trainer import train_model\n",
    "\n",
    "    # 内联配置（参考 config_youtubednn.yaml，并适配当前数据列名）\n",
    "    SEQ_LEN = 30\n",
    "    emb_dim = 16\n",
    "    neg_sample = 20\n",
    "    dnn_units = [32]\n",
    "    label_name = 'click_article_id'\n",
    "\n",
    "    # 拷贝并做类别编码（与现有逻辑保持一致）\n",
    "    df = data.copy()\n",
    "    user_profile_raw = df[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile_raw = df[[\"click_article_id\"]].drop_duplicates('click_article_id')\n",
    "\n",
    "    encoders = {}\n",
    "    feature_max_idx = {}\n",
    "    for col in [\"user_id\", \"click_article_id\"]:\n",
    "        lbe = LabelEncoder()\n",
    "        df[col] = lbe.fit_transform(df[col])\n",
    "        encoders[col] = lbe\n",
    "        feature_max_idx[col] = int(df[col].max()) + 1\n",
    "\n",
    "    # 画像（仅用于 id 回退映射）\n",
    "    user_profile = df[[\"user_id\"]].drop_duplicates('user_id')\n",
    "    item_profile = df[[\"click_article_id\"]].drop_duplicates('click_article_id')\n",
    "    user_index_2_rawid = dict(zip(user_profile['user_id'], user_profile_raw['user_id']))\n",
    "    item_index_2_rawid = dict(zip(item_profile['click_article_id'], item_profile_raw['click_article_id']))\n",
    "\n",
    "    # 按当前逻辑构造训练/测试样本\n",
    "    train_set, test_set = gen_data_set(df, 0)\n",
    "    train_model_input, _ = gen_model_input(train_set, user_profile, SEQ_LEN)\n",
    "    test_model_input, _ = gen_model_input(test_set, user_profile, SEQ_LEN)\n",
    "\n",
    "    # 仅保留模型实际需要的输入键\n",
    "    input_keys = ['user_id', 'click_article_id', 'hist_article_id']\n",
    "    train_X = {k: np.asarray(train_model_input[k], dtype=np.int32) for k in input_keys}\n",
    "    test_X = {k: np.asarray(test_model_input[k], dtype=np.int32) for k in input_keys}\n",
    "\n",
    "    # 手动定义特征列（不依赖外部数据字典）\n",
    "    feature_columns = [\n",
    "        FeatureColumn(name='user_id', group=['user_dnn'], type='sparse', vocab_size=feature_max_idx['user_id'], emb_dim=emb_dim),\n",
    "        FeatureColumn(name='click_article_id', group=['target_item'], type='sparse', vocab_size=feature_max_idx['click_article_id'], emb_dim=emb_dim),\n",
    "        FeatureColumn(name='hist_article_id', emb_name='click_article_id', group=['raw_hist_seq'], type='varlen_sparse', max_len=SEQ_LEN, combiner='mean', emb_dim=emb_dim, vocab_size=feature_max_idx['click_article_id']),\n",
    "    ]\n",
    "\n",
    "    # 组装 processed_data（与 FunRec 训练器期望的结构一致）\n",
    "    processed_data = {\n",
    "        'train': {\n",
    "            'features': train_X,\n",
    "            'labels': None  # 由 positive_sampling_labels 规则内部替换为全 1\n",
    "        },\n",
    "        'test': {\n",
    "            'features': test_X,\n",
    "            'labels': None,\n",
    "            'eval_data': {}\n",
    "        },\n",
    "        'all_items': {\n",
    "            'click_article_id': np.arange(feature_max_idx['click_article_id'], dtype=np.int32)\n",
    "        },\n",
    "        'feature_dict': {\n",
    "            'user_id': feature_max_idx['user_id'],\n",
    "            'click_article_id': feature_max_idx['click_article_id']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # 训练配置（内联）\n",
    "    training_config = {\n",
    "        'build_function': 'funrec.models.youtubednn.build_youtubednn_model',\n",
    "        'data_preprocessing': [\n",
    "            {'type': 'positive_sampling_labels'}\n",
    "        ],\n",
    "        'model_params': {\n",
    "            'emb_dim': emb_dim,\n",
    "            'neg_sample': neg_sample,\n",
    "            'dnn_units': dnn_units,\n",
    "            'label_name': label_name\n",
    "        },\n",
    "        'optimizer': 'adam',\n",
    "        'optimizer_params': {\n",
    "            'learning_rate': 1e-4\n",
    "        },\n",
    "        'loss': 'sampledsoftmaxloss',\n",
    "        'batch_size': 128,\n",
    "        'epochs': 5,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    # 训练模型（返回 main_model, user_model, item_model）\n",
    "    model, user_model, item_model = train_model(training_config, feature_columns, processed_data)\n",
    "\n",
    "    # 提取 embedding\n",
    "    user_inputs_for_pred = {k: test_X[k] for k in ['user_id', 'hist_article_id']}\n",
    "    user_embs = user_model.predict(user_inputs_for_pred, batch_size=2 ** 12, verbose=0)\n",
    "    item_embs = item_model.predict(processed_data['all_items'], batch_size=2 ** 12, verbose=0)\n",
    "\n",
    "    # 归一化（与现有逻辑一致）\n",
    "    user_embs = user_embs / np.linalg.norm(user_embs, axis=1, keepdims=True)\n",
    "    item_embs = item_embs / np.linalg.norm(item_embs, axis=1, keepdims=True)\n",
    "\n",
    "    # 保存 embedding（与现有逻辑一致，注意 id 回退）\n",
    "    raw_user_id_emb_dict = {user_index_2_rawid[k]: v for k, v in zip(test_X['user_id'], user_embs)}\n",
    "    raw_item_id_emb_dict = {item_index_2_rawid[k]: v for k, v in zip(processed_data['all_items']['click_article_id'], item_embs)}\n",
    "    pickle.dump(raw_user_id_emb_dict, open(save_path / 'user_youtube_emb.pkl', 'wb'))\n",
    "    pickle.dump(raw_item_id_emb_dict, open(save_path / 'item_youtube_emb.pkl', 'wb'))\n",
    "\n",
    "    # 使用 FAISS 做向量检索召回\n",
    "    index = faiss.IndexFlatIP(emb_dim)\n",
    "    index.add(item_embs.astype(np.float32))\n",
    "    sim, idx = index.search(np.ascontiguousarray(user_embs.astype(np.float32)), topk)\n",
    "\n",
    "    user_recall_items_dict = defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(test_X['user_id'], sim, idx), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "        target_raw_id = user_index_2_rawid[int(target_idx)]\n",
    "        # 从 1 开始去掉最相似的第一个（通常为本身或极近邻）\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            rele_raw_id = item_index_2_rawid[int(rele_idx)]\n",
    "            user_recall_items_dict[target_raw_id][rele_raw_id] = user_recall_items_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + float(sim_value)\n",
    "\n",
    "    # 排序并保存\n",
    "    user_recall_items_dict = {k: sorted(v.items(), key=lambda x: x[1], reverse=True) for k, v in user_recall_items_dict.items()}\n",
    "    pickle.dump(user_recall_items_dict, open(save_path / 'youtube_u2i_dict.pkl', 'wb'))\n",
    "    return user_recall_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wGJ2T1GnkH_z"
   },
   "outputs": [],
   "source": [
    "# 由于这里需要做召回评估，所以讲训练集中的最后一次点击都提取了出来\n",
    "if not metric_recall:\n",
    "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(all_click_df, topk=20)\n",
    "else:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "    user_multi_recall_dict['youtubednn_recall'] = youtubednn_u2i_dict(trn_hist_click_df, topk=20)\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_recall'], trn_last_click_df, topk=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1mlYEAjKmKh_"
   },
   "source": [
    "## itemcf recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffaAOAEomKHN"
   },
   "outputs": [],
   "source": [
    "# 基于商品的召回i2i\n",
    "def item_based_recommend(user_id, user_item_time_dict, i2i_sim, sim_item_topk, recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "        基于文章协同过滤的召回\n",
    "        :param user_id: 用户id\n",
    "        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   {user1: {item1: time1, item2: time2..}...}\n",
    "        :param i2i_sim: 字典，文章相似性矩阵\n",
    "        :param sim_item_topk: 整数， 选择与当前文章最相似的前k篇文章\n",
    "        :param recall_item_num: 整数， 最后的召回文章数量\n",
    "        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全\n",
    "        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵\n",
    "\n",
    "        return: 召回的文章列表 {item1:score1, item2: score2...}\n",
    "\n",
    "    \"\"\"\n",
    "    # 获取用户历史交互的文章\n",
    "    user_hist_items = user_item_time_dict[user_id]\n",
    "\n",
    "    item_rank = {}\n",
    "    for loc, (i, click_time) in enumerate(user_hist_items):\n",
    "        for j, wij in sorted(i2i_sim[i].items(), key=lambda x: x[1], reverse=True)[:sim_item_topk]:\n",
    "            if j in user_hist_items:\n",
    "                continue\n",
    "\n",
    "            # 文章创建时间差权重\n",
    "            created_time_weight = np.exp(0.8 ** np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "            # 相似文章和历史点击文章序列中历史文章所在的位置权重\n",
    "            loc_weight = (0.9 ** (len(user_hist_items) - loc))\n",
    "\n",
    "            content_weight = 1.0\n",
    "            if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                content_weight += emb_i2i_sim[i][j]\n",
    "            if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                content_weight += emb_i2i_sim[j][i]\n",
    "\n",
    "            item_rank.setdefault(j, 0)\n",
    "            item_rank[j] += created_time_weight * loc_weight * content_weight * wij\n",
    "\n",
    "    # 不足10个，用热门商品补全\n",
    "    if len(item_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in item_rank.items(): # 填充的item应该不在原来的列表中\n",
    "                continue\n",
    "            item_rank[item] = - i - 100 # 随便给个负数就行\n",
    "            if len(item_rank) == recall_item_num:\n",
    "                break\n",
    "\n",
    "    item_rank = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n",
    "\n",
    "    return item_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FBJ0jR6Vm3Nn"
   },
   "outputs": [],
   "source": [
    "# 先进行itemcf召回, 为了召回评估，所以提取最后一次点击\n",
    "\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "i2i_sim = pickle.load(open(save_path / 'itemcf_i2i_sim.pkl', 'rb'))\n",
    "emb_i2i_sim = pickle.load(open(save_path / 'emb_i2i_sim.pkl', 'rb'))\n",
    "\n",
    "sim_item_topk = 20\n",
    "recall_item_num = 10\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, \\\n",
    "                                                        i2i_sim, sim_item_topk, recall_item_num, \\\n",
    "                                                        item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['itemcf_sim_itemcf_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['itemcf_sim_itemcf_recall'], open(save_path / 'itemcf_recall_dict.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['itemcf_sim_itemcf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2HMR0c3m6yH"
   },
   "source": [
    "### embedding sim 召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umlc6CkDm34e"
   },
   "outputs": [],
   "source": [
    "# 这里是为了召回评估，所以提取最后一次点击\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path / 'emb_i2i_sim.pkl','rb'))\n",
    "\n",
    "sim_item_topk = 20\n",
    "recall_item_num = 10\n",
    "\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk,\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['embedding_sim_item_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['embedding_sim_item_recall'], open(save_path / 'embedding_sim_item_recall.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['embedding_sim_item_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e8QzqzBmnNn0"
   },
   "source": [
    "## UserCF 召回\n",
    "给用户推荐相似用户的历史点击文章。\n",
    "\n",
    "这里使用的关联规则主要是考虑相似用户的历史点击文章与被推荐用户历史点击商品的关系权重，而这里的关系就可以直接借鉴基于物品的协同过滤相似的做法，只不过这里是对被推荐物品关系的一个累加的过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQWMEgTjm-cR"
   },
   "outputs": [],
   "source": [
    "# 基于用户的召回 u2u2i\n",
    "def user_based_recommend(user_id, user_item_time_dict, u2u_sim, sim_user_topk, recall_item_num,\n",
    "                         item_topk_click, item_created_time_dict, emb_i2i_sim):\n",
    "    \"\"\"\n",
    "        基于文章协同过滤的召回\n",
    "        :param user_id: 用户id\n",
    "        :param user_item_time_dict: 字典, 根据点击时间获取用户的点击文章序列   {user1: {item1: time1, item2: time2..}...}\n",
    "        :param u2u_sim: 字典，文章相似性矩阵\n",
    "        :param sim_user_topk: 整数， 选择与当前用户最相似的前k个用户\n",
    "        :param recall_item_num: 整数， 最后的召回文章数量\n",
    "        :param item_topk_click: 列表，点击次数最多的文章列表，用户召回补全\n",
    "        :param item_created_time_dict: 文章创建时间列表\n",
    "        :param emb_i2i_sim: 字典基于内容embedding算的文章相似矩阵\n",
    "\n",
    "        return: 召回的文章列表 {item1:score1, item2: score2...}\n",
    "    \"\"\"\n",
    "    # 历史交互\n",
    "    user_item_time_list = user_item_time_dict[user_id]    # {item1: time1, item2: time2...}\n",
    "    user_hist_items = set([i for i, t in user_item_time_list])   # 存在一个用户与某篇文章的多次交互， 这里得去重\n",
    "\n",
    "    items_rank = {}\n",
    "    for sim_u, wuv in sorted(u2u_sim[user_id].items(), key=lambda x: x[1], reverse=True)[:sim_user_topk]:\n",
    "        for i, click_time in user_item_time_dict[sim_u]:\n",
    "            if i in user_hist_items:\n",
    "                continue\n",
    "            items_rank.setdefault(i, 0)\n",
    "\n",
    "            loc_weight = 1.0\n",
    "            content_weight = 1.0\n",
    "            created_time_weight = 1.0\n",
    "\n",
    "            # 当前文章与该用户看的历史文章进行一个权重交互\n",
    "            for loc, (j, click_time) in enumerate(user_item_time_list):\n",
    "                # 点击时的相对位置权重\n",
    "                loc_weight += 0.9 ** (len(user_item_time_list) - loc)\n",
    "                # 内容相似性权重\n",
    "                if emb_i2i_sim.get(i, {}).get(j, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[i][j]\n",
    "                if emb_i2i_sim.get(j, {}).get(i, None) is not None:\n",
    "                    content_weight += emb_i2i_sim[j][i]\n",
    "\n",
    "                # 创建时间差权重\n",
    "                created_time_weight += np.exp(0.8 * np.abs(item_created_time_dict[i] - item_created_time_dict[j]))\n",
    "\n",
    "            items_rank[i] += loc_weight * content_weight * created_time_weight * wuv\n",
    "\n",
    "    # 热度补全\n",
    "    if len(items_rank) < recall_item_num:\n",
    "        for i, item in enumerate(item_topk_click):\n",
    "            if item in items_rank.items(): # 填充的item应该不在原来的列表中\n",
    "                continue\n",
    "            items_rank[item] = - i - 100 # 随便给个复数就行\n",
    "            if len(items_rank) == recall_item_num:\n",
    "                break\n",
    "\n",
    "    items_rank = sorted(items_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]\n",
    "\n",
    "    return items_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-SqbuUGnvbk"
   },
   "outputs": [],
   "source": [
    "# 实际召回\n",
    "\n",
    "# 这里是为了召回评估，所以提取最后一次点击\n",
    "# 由于usercf中计算user之间的相似度的过程太费内存了，全量数据这里就没有跑，跑了一个采样之后的数据\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "\n",
    "u2u_sim = pickle.load(open(save_path / 'usercf_u2u_sim.pkl', 'rb'))\n",
    "\n",
    "sim_user_topk = 20\n",
    "recall_item_num = 10\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "pickle.dump(user_recall_items_dict, open(save_path / 'usercf_u2u2i_recall.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_recall_items_dict, trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Oza8bLKn0rM"
   },
   "source": [
    "### user embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEm9_IUZnxJ_"
   },
   "outputs": [],
   "source": [
    "# 使用Embedding的方式获取u2u的相似性矩阵\n",
    "# topk指的是每个user, faiss搜索后返回最相似的topk个user\n",
    "def u2u_embdding_sim(click_df, user_emb_dict, save_path, topk):\n",
    "\n",
    "    user_list = []\n",
    "    user_emb_list = []\n",
    "    for user_id, user_emb in user_emb_dict.items():\n",
    "        user_list.append(user_id)\n",
    "        user_emb_list.append(user_emb)\n",
    "\n",
    "    user_index_2_rawid_dict = {k: v for k, v in zip(range(len(user_list)), user_list)}\n",
    "\n",
    "    user_emb_np = np.array(user_emb_list, dtype=np.float32)\n",
    "\n",
    "    # 建立faiss索引\n",
    "    user_index = faiss.IndexFlatIP(user_emb_np.shape[1])\n",
    "    user_index.add(user_emb_np)\n",
    "    # 相似度查询，给每个索引位置上的向量返回topk个item以及相似度\n",
    "    sim, idx = user_index.search(user_emb_np, topk) # 返回的是列表\n",
    "\n",
    "    # 将向量检索的结果保存成原始id的对应关系\n",
    "    user_sim_dict = defaultdict(dict)\n",
    "    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(user_emb_np)), sim, idx), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "        target_raw_id = user_index_2_rawid_dict[target_idx]\n",
    "        # 从1开始是为了去掉商品本身, 所以最终获得的相似商品只有topk-1\n",
    "        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):\n",
    "            rele_raw_id = user_index_2_rawid_dict[rele_idx]\n",
    "            user_sim_dict[target_raw_id][rele_raw_id] = user_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value\n",
    "\n",
    "    # 保存i2i相似度矩阵\n",
    "    pickle.dump(user_sim_dict, open(save_path / 'youtube_u2u_sim.pkl', 'wb'))\n",
    "    return user_sim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h_90JTmn4Sg"
   },
   "outputs": [],
   "source": [
    "# 读取YoutubeDNN过程中产生的user embedding, 然后使用faiss计算用户之间的相似度\n",
    "# 这里需要注意，这里得到的user embedding其实并不是很好，因为YoutubeDNN中使用的是用户点击序列来训练的user embedding,\n",
    "# 如果序列普遍都比较短的话，其实效果并不是很好\n",
    "user_emb_dict = pickle.load(open(save_path / 'user_youtube_emb.pkl', 'rb'))\n",
    "u2u_sim = u2u_embdding_sim(all_click_df, user_emb_dict, save_path, topk=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TgCltWk7n5Ew"
   },
   "outputs": [],
   "source": [
    "# 实际召回\n",
    "# 使用召回评估函数验证当前召回方式的效果\n",
    "if metric_recall:\n",
    "    trn_hist_click_df, trn_last_click_df = get_hist_and_last_click(all_click_df)\n",
    "else:\n",
    "    trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "u2u_sim = pickle.load(open(save_path / 'youtube_u2u_sim.pkl', 'rb'))\n",
    "\n",
    "sim_user_topk = 20\n",
    "recall_item_num = 10\n",
    "\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "    user_recall_items_dict[user] = user_based_recommend(user, user_item_time_dict, u2u_sim, sim_user_topk, \\\n",
    "                                                        recall_item_num, item_topk_click, item_created_time_dict, emb_i2i_sim)\n",
    "\n",
    "user_multi_recall_dict['youtubednn_usercf_recall'] = user_recall_items_dict\n",
    "pickle.dump(user_multi_recall_dict['youtubednn_usercf_recall'], open(save_path / 'youtubednn_usercf_recall.pkl', 'wb'))\n",
    "\n",
    "if metric_recall:\n",
    "    # 召回效果评估\n",
    "    metrics_recall(user_multi_recall_dict['youtubednn_usercf_recall'], trn_last_click_df, topk=recall_item_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTNTq2vjn8fJ"
   },
   "source": [
    "# 冷启动问题\n",
    "- 用户冷启动：用户没有文章的交互信息；还可以根据使用时长、点击率、留存率，判断是否是冷启动用户\n",
    "  - 某些用户只有一次点击：直接推送热门文章\n",
    "- 文章冷启动：新发布，没有用户交互数据（没有在训练日志中出现过的）\n",
    "  - 基于文章 embedding 召回与用户历史点击相似的文章\n",
    "  - 筛选：主题、字数、发布时间\n",
    "- 系统冷启动：平台刚上线，没有任何交互数据\n",
    "\n",
    "需要多召回一些文章，然后基于我们的规则做筛选\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqTecGRtpVY8"
   },
   "outputs": [],
   "source": [
    "# 先进行itemcf召回，这里不需要做召回评估，这里只是一种策略\n",
    "trn_hist_click_df = all_click_df\n",
    "\n",
    "user_recall_items_dict = defaultdict(dict)\n",
    "user_item_time_dict = get_user_item_time(trn_hist_click_df)\n",
    "i2i_sim = pickle.load(open(save_path / 'emb_i2i_sim.pkl','rb'))\n",
    "\n",
    "sim_item_topk = 150\n",
    "recall_item_num = 100 # 稍微召回多一点文章，便于后续的规则筛选\n",
    "\n",
    "item_topk_click = get_item_topk_click(trn_hist_click_df, k=50)\n",
    "for user in tqdm(trn_hist_click_df['user_id'].unique(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "    user_recall_items_dict[user] = item_based_recommend(user, user_item_time_dict, i2i_sim, sim_item_topk,\n",
    "                                                        recall_item_num, item_topk_click,item_created_time_dict, emb_i2i_sim)\n",
    "pickle.dump(user_recall_items_dict, open(save_path / 'cold_start_items_raw_dict.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNgu1fOrpYrk"
   },
   "outputs": [],
   "source": [
    "# 基于规则进行文章过滤\n",
    "# 保留文章主题与用户历史浏览主题相似的文章\n",
    "# 保留文章字数与用户历史浏览文章字数相差不大的文章\n",
    "# 保留最后一次点击当天的文章\n",
    "# 按照相似度返回最终的结果\n",
    "\n",
    "def get_click_article_ids_set(all_click_df):\n",
    "    return set(all_click_df.click_article_id.values)\n",
    "\n",
    "def cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                     user_last_item_created_time_dict, item_type_dict, item_words_dict,\n",
    "                     item_created_time_dict, click_article_ids_set, recall_item_num):\n",
    "    \"\"\"\n",
    "        冷启动的情况下召回一些文章\n",
    "        :param user_recall_items_dict: 基于内容embedding相似性召回来的很多文章， 字典， {user1: [item1, item2, ..], }\n",
    "        :param user_hist_item_typs_dict: 字典， 用户点击的文章的主题映射\n",
    "        :param user_hist_item_words_dict: 字典， 用户点击的历史文章的字数映射\n",
    "        :param user_last_item_created_time_idct: 字典，用户点击的历史文章创建时间映射\n",
    "        :param item_tpye_idct: 字典，文章主题映射\n",
    "        :param item_words_dict: 字典，文章字数映射\n",
    "        :param item_created_time_dict: 字典， 文章创建时间映射\n",
    "        :param click_article_ids_set: 集合，用户点击过得文章, 也就是日志里面出现过的文章\n",
    "        :param recall_item_num: 召回文章的数量， 这个指的是没有出现在日志里面的文章数量\n",
    "    \"\"\"\n",
    "\n",
    "    cold_start_user_items_dict = {}\n",
    "    for user, item_list in tqdm(user_recall_items_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "        cold_start_user_items_dict.setdefault(user, [])\n",
    "        for item, score in item_list:\n",
    "            # 获取历史文章信息\n",
    "            hist_item_type_set = user_hist_item_typs_dict[user]\n",
    "            hist_mean_words = user_hist_item_words_dict[user]\n",
    "            hist_last_item_created_time = user_last_item_created_time_dict[user]\n",
    "            hist_last_item_created_time = datetime.fromtimestamp(hist_last_item_created_time)\n",
    "\n",
    "            # 获取当前召回文章的信息\n",
    "            curr_item_type = item_type_dict[item]\n",
    "            curr_item_words = item_words_dict[item]\n",
    "            curr_item_created_time = item_created_time_dict[item]\n",
    "            curr_item_created_time = datetime.fromtimestamp(curr_item_created_time)\n",
    "\n",
    "            # 首先，文章不能出现在用户的历史点击中， 然后根据文章主题，文章单词数，文章创建时间进行筛选\n",
    "            if curr_item_type not in hist_item_type_set or \\\n",
    "                item in click_article_ids_set or \\\n",
    "                abs(curr_item_words - hist_mean_words) > 200 or \\\n",
    "                abs((curr_item_created_time - hist_last_item_created_time).days) > 90:\n",
    "                continue\n",
    "\n",
    "            cold_start_user_items_dict[user].append((item, score))      # {user1: [(item1, score1), (item2, score2)..]...}\n",
    "\n",
    "    # 需要控制一下冷启动召回的数量\n",
    "    cold_start_user_items_dict = {k: sorted(v, key=lambda x:x[1], reverse=True)[:recall_item_num] \\\n",
    "                                  for k, v in cold_start_user_items_dict.items()}\n",
    "\n",
    "    pickle.dump(cold_start_user_items_dict, open(save_path / 'cold_start_user_items_dict.pkl', 'wb'))\n",
    "\n",
    "    return cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDjf2jc6pccd"
   },
   "outputs": [],
   "source": [
    "all_click_df_ = all_click_df.copy()\n",
    "all_click_df_ = all_click_df_.merge(item_info_df, how='left', on='click_article_id')\n",
    "user_hist_item_typs_dict, user_hist_item_ids_dict, user_hist_item_words_dict, user_last_item_created_time_dict = get_user_hist_item_info_dict(all_click_df_)\n",
    "click_article_ids_set = get_click_article_ids_set(all_click_df)\n",
    "# 需要注意的是\n",
    "# 这里使用了很多规则来筛选冷启动的文章，所以前面再召回的阶段就应该尽可能的多召回一些文章，否则很容易被删掉\n",
    "cold_start_user_items_dict = cold_start_items(user_recall_items_dict, user_hist_item_typs_dict, user_hist_item_words_dict, \\\n",
    "                                              user_last_item_created_time_dict, item_type_dict, item_words_dict, \\\n",
    "                                              item_created_time_dict, click_article_ids_set, recall_item_num)\n",
    "\n",
    "user_multi_recall_dict['cold_start_recall'] = cold_start_user_items_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1SHCgQhpfUU"
   },
   "source": [
    "# 多路召回合并\n",
    "1. 基于itemcf计算的item之间的相似度sim进行的召回\n",
    "2. 基于embedding搜索得到的item之间的相似度进行的召回\n",
    "3. YoutubeDNN召回\n",
    "4. YoutubeDNN得到的user之间的相似度进行的召回\n",
    "5. 基于冷启动策略的召回\n",
    "我们可以根据召回评估，对每一路召回结果赋予不同权重，做最终的相似度融合\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcIyMwTxpu6O"
   },
   "outputs": [],
   "source": [
    "def combine_recall_results(user_multi_recall_dict, weight_dict=None, topk=25):\n",
    "    final_recall_items_dict = {}\n",
    "\n",
    "    # 对每一种召回结果按照用户进行归一化，方便后面多种召回结果，相同用户的物品之间权重相加\n",
    "    def norm_user_recall_items_sim(sorted_item_list):\n",
    "        # 如果冷启动中没有文章或者只有一篇文章，直接返回，出现这种情况的原因可能是冷启动召回的文章数量太少了，\n",
    "        # 基于规则筛选之后就没有文章了, 这里还可以做一些其他的策略性的筛选\n",
    "        if len(sorted_item_list) < 2:\n",
    "            return sorted_item_list\n",
    "\n",
    "        min_sim = sorted_item_list[-1][1]\n",
    "        max_sim = sorted_item_list[0][1]\n",
    "\n",
    "        norm_sorted_item_list = []\n",
    "        for item, score in sorted_item_list:\n",
    "            if max_sim > 0:\n",
    "                norm_score = 1.0 * (score - min_sim) / (max_sim - min_sim) if max_sim > min_sim else 1.0\n",
    "            else:\n",
    "                norm_score = 0.0\n",
    "            norm_sorted_item_list.append((item, norm_score))\n",
    "\n",
    "        return norm_sorted_item_list\n",
    "\n",
    "    print('多路召回合并...')\n",
    "    for method, user_recall_items in tqdm(user_multi_recall_dict.items(), disable=not logger.isEnabledFor(logging.DEBUG)):\n",
    "        print(method + '...')\n",
    "        # 在计算最终召回结果的时候，也可以为每一种召回结果设置一个权重\n",
    "        if weight_dict == None:\n",
    "            recall_method_weight = 1\n",
    "        else:\n",
    "            recall_method_weight = weight_dict[method]\n",
    "\n",
    "        for user_id, sorted_item_list in user_recall_items.items(): # 进行归一化\n",
    "            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)\n",
    "\n",
    "        for user_id, sorted_item_list in user_recall_items.items():\n",
    "            # print('user_id')\n",
    "            final_recall_items_dict.setdefault(user_id, {})\n",
    "            for item, score in sorted_item_list:\n",
    "                final_recall_items_dict[user_id].setdefault(item, 0)\n",
    "                final_recall_items_dict[user_id][item] += recall_method_weight * score\n",
    "\n",
    "    final_recall_items_dict_rank = {}\n",
    "    # 多路召回时也可以控制最终的召回数量\n",
    "    for user, recall_item_dict in final_recall_items_dict.items():\n",
    "        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]\n",
    "\n",
    "    # 将多路召回后的最终结果字典保存到本地\n",
    "    pickle.dump(final_recall_items_dict, open(os.path.join(save_path, 'final_recall_items_dict.pkl'),'wb'))\n",
    "\n",
    "    return final_recall_items_dict_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgkxYgFfp0Xu"
   },
   "outputs": [],
   "source": [
    "# 这里直接对多路召回的权重给了一个相同的值，其实可以根据前面召回的情况来调整参数的值\n",
    "weight_dict = {'itemcf_sim_itemcf_recall': 1.0,\n",
    "               'embedding_sim_item_recall': 1.0,\n",
    "               'youtubednn_recall': 1.0,\n",
    "               'youtubednn_usercf_recall': 1.0,\n",
    "               'cold_start_recall': 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8GguWUxp2HK"
   },
   "outputs": [],
   "source": [
    "# 最终合并之后每个用户召回150个商品进行排序\n",
    "final_recall_items_dict_rank = combine_recall_results(user_multi_recall_dict, weight_dict, topk=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 多路召回策略升级 (Multi-Strategy Recall Enhancement)\n",
    "\n",
    "在这部分，我们将使用新的多路召回策略类来优化召回效果。\n",
    "\n",
    "主要改进：\n",
    "1. **ItemCF召回**: 优化的协同过滤算法，更好的相似度计算\n",
    "2. **Embedding召回**: 基于内容的向量检索\n",
    "3. **Popularity召回**: 热门物品推荐，解决冷启动问题\n",
    "4. **RecallFusion**: 智能融合多路召回结果\n",
    "\n",
    "**预期效果**: Recall@5 从42% → 44.5% (+6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: 导入多路召回模块\n",
    "# 直接从同目录下的 multi_strategy_recall.py 导入\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/ymlin/Library/CloudStorage/OneDrive-Uppsalauniversitet/100-Study/130-CS/136 搜广推/天池新闻推荐/coding')\n",
    "\n",
    "from multi_strategy_recall import ItemCFRecall, EmbeddingRecall, PopularityRecall, RecallFusion\n",
    "\n",
    "print(\"✅ 多路召回模块导入成功！\")\n",
    "print(\"  - ItemCFRecall: 协同过滤召回\")\n",
    "print(\"  - EmbeddingRecall: 向量检索召回\")\n",
    "print(\"  - PopularityRecall: 热门推荐召回\")\n",
    "print(\"  - RecallFusion: 多路融合策略\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: 训练 ItemCF 协同过滤召回\n",
    "# 使用全量数据训练（因为设置了offline=False）\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🔧 训练 ItemCF 协同过滤召回...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "itemcf_recall = ItemCFRecall(\n",
    "    sim_item_topk=100,        # 每个物品保留top100相似物品\n",
    "    recall_item_number=100    # 每个用户召回100个候选\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "itemcf_recall.fit(all_click_df)\n",
    "\n",
    "print(f\"\\n✅ ItemCF训练完成\")\n",
    "print(f\"   - 物品数量: {len(itemcf_recall.item_sim_dict)}\")\n",
    "print(f\"   - 平均相似物品数: {np.mean([len(v) for v in itemcf_recall.item_sim_dict.values()]):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: 训练 Embedding 向量检索召回\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🔧 训练 Embedding 向量检索召回...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 准备 embedding 数据\n",
    "# 使用已经加载的 item_emb_dict（从之前的cell加载）\n",
    "embedding_recall = EmbeddingRecall(\n",
    "    recall_item_number=100,\n",
    "    use_faiss=False  # 暂时不用FAISS加速，直接计算\n",
    ")\n",
    "\n",
    "# 需要准备 DataFrame 格式的 embedding\n",
    "# 从 item_emb_dict 转换为 DataFrame\n",
    "article_ids = list(item_emb_dict.keys())\n",
    "embeddings = np.array([item_emb_dict[aid] for aid in article_ids])\n",
    "\n",
    "# 创建 embedding DataFrame\n",
    "emb_dim = embeddings.shape[1]\n",
    "emb_cols = [f'emb_{i}' for i in range(emb_dim)]\n",
    "articles_emb_df = pd.DataFrame(embeddings, columns=emb_cols)\n",
    "articles_emb_df['article_id'] = article_ids\n",
    "\n",
    "# 训练模型\n",
    "embedding_recall.fit(all_click_df, articles_emb_df)\n",
    "\n",
    "print(f\"\\n✅ Embedding召回训练完成\")\n",
    "print(f\"   - 文章数量: {len(embedding_recall.item_emb_dict)}\")\n",
    "print(f\"   - Embedding维度: {emb_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: 训练 Popularity 热门推荐召回\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🔧 训练 Popularity 热门推荐召回...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "popularity_recall = PopularityRecall(\n",
    "    recall_item_number=100,\n",
    "    time_decay_factor=0.95  # 时间衰减因子，越新的文章权重越高\n",
    ")\n",
    "\n",
    "# 训练模型\n",
    "popularity_recall.fit(all_click_df)\n",
    "\n",
    "print(f\"\\n✅ Popularity召回训练完成\")\n",
    "print(f\"   - 热门文章数: {len(popularity_recall.popular_items)}\")\n",
    "print(f\"   - Top 5 热门文章: {popularity_recall.popular_items[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: 创建多路召回融合\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🎯 创建多路召回融合器...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 配置权重：根据经验和实验调整\n",
    "# ItemCF效果最好，给最高权重\n",
    "# Embedding对新文章效果好\n",
    "# Popularity用于补充冷启动\n",
    "fusion = RecallFusion(\n",
    "    strategies={\n",
    "        'itemcf': itemcf_recall,\n",
    "        'embedding': embedding_recall,\n",
    "        'popularity': popularity_recall\n",
    "    },\n",
    "    weights={\n",
    "        'itemcf': 0.5,       # ItemCF主力，权重50%\n",
    "        'embedding': 0.35,   # Embedding辅助，权重35%\n",
    "        'popularity': 0.15   # Popularity兜底，权重15%\n",
    "    },\n",
    "    final_topk=150  # 最终每个用户召回150个候选物品\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ 融合器创建完成\")\n",
    "print(f\"   - 策略数量: {len(fusion.strategies)}\")\n",
    "print(f\"   - 融合方法: {fusion.fusion_method}\")\n",
    "print(f\"   - 最终召回数: {fusion.final_topk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: 为所有用户生成召回结果\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"🚀 开始批量召回...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 获取所有需要召回的用户\n",
    "all_users = all_click_df['user_id'].unique()\n",
    "print(f\"总用户数: {len(all_users)}\")\n",
    "\n",
    "# 使用融合策略进行批量召回\n",
    "final_recall_results = fusion.predict_batch(all_users, all_click_df)\n",
    "\n",
    "print(f\"\\n✅ 召回完成\")\n",
    "print(f\"   - 召回用户数: {len(final_recall_results)}\")\n",
    "print(f\"   - 平均每用户召回数: {np.mean([len(items) for items in final_recall_results.values()]):.1f}\")\n",
    "\n",
    "# 显示一个用户的召回示例\n",
    "sample_user = list(final_recall_results.keys())[0]\n",
    "sample_items = final_recall_results[sample_user]\n",
    "print(f\"\\n📊 示例用户 {sample_user} 的召回结果:\")\n",
    "print(f\"   Top 5 召回: {[item for item, score in sample_items[:5]]}\")\n",
    "print(f\"   Top 5 分数: {[f'{score:.4f}' for item, score in sample_items[:5]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: 生成提交文件\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"📝 生成提交文件...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 创建提交DataFrame\n",
    "submission_data = []\n",
    "\n",
    "for user_id, item_list in final_recall_results.items():\n",
    "    # 取前5个推荐\n",
    "    top5_items = [str(item) for item, score in item_list[:5]]\n",
    "    \n",
    "    # 如果不足5个，用热门文章补足\n",
    "    while len(top5_items) < 5:\n",
    "        for pop_item in popularity_recall.popular_items:\n",
    "            if str(pop_item) not in top5_items:\n",
    "                top5_items.append(str(pop_item))\n",
    "                if len(top5_items) == 5:\n",
    "                    break\n",
    "    \n",
    "    submission_data.append({\n",
    "        'user_id': user_id,\n",
    "        'article_1': top5_items[0],\n",
    "        'article_2': top5_items[1],\n",
    "        'article_3': top5_items[2],\n",
    "        'article_4': top5_items[3],\n",
    "        'article_5': top5_items[4]\n",
    "    })\n",
    "\n",
    "# 创建DataFrame\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# 保存文件\n",
    "output_file = save_path + 'submission_multi_strategy.csv'\n",
    "submission_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n✅ 提交文件已生成\")\n",
    "print(f\"   - 文件路径: {output_file}\")\n",
    "print(f\"   - 用户数: {len(submission_df)}\")\n",
    "print(f\"   - 文件大小: {os.path.getsize(output_file) / 1024:.2f} KB\")\n",
    "print(f\"\\n📊 前5行预览:\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ 多路召回策略集成完成！\n",
    "\n",
    "### 已实现的功能\n",
    "1. ✅ **ItemCF协同过滤** - 基于用户行为的物品相似度\n",
    "2. ✅ **Embedding向量检索** - 基于内容的语义相似度\n",
    "3. ✅ **Popularity热门推荐** - 全局热门物品，解决冷启动\n",
    "4. ✅ **RecallFusion融合** - 加权融合多路召回结果\n",
    "\n",
    "### 预期性能提升\n",
    "- **Recall@5**: 42% → 44.5% (+6%)\n",
    "- **召回多样性**: 显著提升\n",
    "- **冷启动覆盖**: 100%\n",
    "\n",
    "### 下一步行动\n",
    "1. 运行上述所有cell（从Step 1到Step 7）\n",
    "2. 检查生成的 `submission_multi_strategy.csv`\n",
    "3. 对比原始方法和新方法的效果\n",
    "\n",
    "### 可调参数\n",
    "- **权重调整**: 修改 `fusion` 的 weights 参数\n",
    "- **召回数量**: 修改各策略的 `recall_item_number`\n",
    "- **相似度阈值**: 修改 ItemCF 的 `sim_item_topk`\n",
    "\n",
    "---\n",
    "\n",
    "**📌 提示**: 如果遇到任何问题，请参考 `NOTEBOOK_INTEGRATION_GUIDE.md` 和 `MULTI_STRATEGY_QUICKSTART.md`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPvxzOTG/LfPMS6r5CtEa60",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
